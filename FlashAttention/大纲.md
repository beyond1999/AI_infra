# FlashAttention 学习路线（大纲 v1）

> 目标：从数学直觉 → PyTorch 原型 → Triton/CUDA Kernel（前向+反向）→ 工程化集成与压测。每节都有**可交付**与**验证标准**。

------

## 0. 先修与工具箱

- 数学：矩阵乘法/范数、log-sum-exp、Softmax 的数值稳定化、掩码（causal & padding）。
- 硬件/并行：GPU 内存层级（寄存器/SMEM/L2/DRAM）、warp/CTA、线程束同步、occupancy、带宽 vs. 计算屋顶线（Roofline）。
- 工具链：PyTorch、Triton 或 CUDA C++（任选其一先上手）、Nsight Compute、nvprof、pytest/pytest-benchmark。
- 可交付：整理一页“术语卡片”，并跑通 `torch.nn.functional.scaled_dot_product_attention` 作为 baseline。

------

## 1. 注意力回顾与痛点

- 标准公式：
  - 令 $Q\in\mathbb{R}^{S\times d}$, $K\in\mathbb{R}^{S\times d}$, $V\in\mathbb{R}^{S\times d_v}$。
  - $S= QK^\top/\sqrt d$；$P=\operatorname{softmax}(S+\text{mask})$；$O=PV$。
- 复杂度/显存：显式物化 $S$ 与 $P$ 需 $\mathcal O(S^2)$ 存储与 IO → 长序列瓶颈。
- 可交付：用 PyTorch 实现**朴素注意力**，测量 2k/4k/8k 序列时峰值显存 & 延迟。

------

## 2. FlashAttention 的核心思想（IO-aware + 在线 Softmax）

- **不物化 $S$ 和 $P$**：把 $K,V$ 沿序列维度切成 tiles（块），流式加载到 SMEM/寄存器中与 $Q$ 块做局部计算。
- **在线（分块）Softmax**：对每一行（query 位置）维护三元组 $(m,,\ell,,o)$：
  - $m$：该行分块得分的**运行最大值**（running max）。
  - $\ell$：该行的**归一化常数**（running sum of exp）。
  - $o$：该行的**输出累积**（running output）。
- 一次处理一个 $K,V$ tile：设当前行对该 tile 的原始得分向量为 $s$，令
   $$m' = \max(m, \max(s)),\quad \alpha = e^{m-m'},$$
   $$\ell' = \alpha,\ell + \sum_j e^{s_j - m'},$$
   $$o' = \frac{\alpha,\ell,o + \sum_j e^{s_j-m'},v_j}{\ell'}.$$
   更新 $(m,\ell,o)\leftarrow(m',\ell',o')$，继续下一个 tile。
- **因果掩码**：对 tile 内超过对角线的列直接赋 $-\infty$（或在计算中跳过）。
- 可交付：写一个**纯 PyTorch 的分块原型**（无自定义 kernel），验证与朴素注意力在 1e-5 相对误差内一致。

------

## 3. 前向原型：PyTorch 分块实现

- 选择块大小（BLOCK_M/BLOCK_N）与 head_dim；对 `dtype=float16/bfloat16` 用 `float32` 累加。
- 实现：
  1. 拆分 $K,V$ 成 tiles；
  2. 对每行维护 $(m,\ell,o)$；
  3. 支持 **causal** 与 **padding mask**；
  4. 单元测试：与基线逐元素比对；随机形状/多头；长序列健壮性。
- 可交付：`flash_attn_chunked.py` + `test_flash_attn_chunked.py`。

------

## 4. Triton/CUDA 前向 Kernel（FlashAttention-Style）

- 网格映射：程序块覆盖 (BLOCK_M rows of Q) ×（沿序列方向遍历 K/V tiles）。
- 内存：Q/K/V 载入共享内存并向量化访问；避免 bank conflict（必要时做 swizzle/padding）。
- 计算：
  - `qk = dot(Q_tile, K_tile^T)`；行内 `max`；`exp`；`sum`；在线归一更新 $(m,\ell,o)$；
  - 写回 $O$。
- 掩码：causal 下对超界列屏蔽；支持 `attn_bias`/RoPE（可选扩展）。
- 可交付：`flash_attn_fwd_triton.cu/py`，含基准对比与 correctness 测试。

------

## 5. 反向传播推导与数值实现

- 记 $O=PV$，$P=\operatorname{softmax}(S)$，$S=QK^\top/\sqrt d$。
- 反向关系：
  - $dV = P^\top,dO$；
  - $dP = dO,V^\top$；
  - Softmax：对每行 $i$，$dS_i = (dP_i - (\sum_j dP_{ij}),P_i) \odot P_i$。
  - $dQ = dS,K/\sqrt d$，$dK = dS^\top,Q/\sqrt d$。
- **无显式 $P$ 的反向**：复用前向保存的 $(m,\ell)$，在遍历 tile 时重构 $P_{ij}=\exp(S_{ij}-m_i)/\ell_i$，流式累计 $dV, dS, dQ, dK$。
- 可交付：`flash_attn_bwd_triton.cu/py` + 单测（gradcheck/数值梯度对照）。

------

## 6. Kernel 优化专章

- **向量化**与对齐：ld/st 128-bit，避免 misaligned；
- **双缓冲**：`cp.async`/软件流水；
- **寄存器/SMEM 配额**与 occupancy 平衡；
- **指令级并行/warp 专化**（进阶）；
- **精度策略**：FP16/BF16 输入，FP32 累加；log-sum-exp 稳定化；溢出/下溢防护。
- 可交付：逐项开启/关闭优化的 A/B 基准表。

------

## 7. Tensor Cores / MMA 路径（进阶）

- 约束：head_dim 与 tile 尺寸对齐 16/32；
- Triton 的 `dot` / CUDA WMMA（`mma.sync`）使用要点；
- BLOCK_K 选择与吞吐；
- 可交付：开启 Tensor Core 的版本与纯 FP32 版本对比（吞吐、误差）。

------

## 8. 工程化集成

- 封装为 **PyTorch Autograd Function**，导出 `flash_attention(q,k,v, causal=False, attn_mask=None)`；
- Multi-Head 支持：批量维/头维展平；
- 与 `torch.compile`/`torch._inductor` 兼容性；
- Fallback：遇到不支持形状/数据类型→退回 PyTorch SDPA。

------

## 9. 基准与对照

- 维度：`S∈{1k,2k,4k,8k,16k}`, `d∈{64,128}`, `n_heads∈{8,16,32}`；
- 对比对象：朴素注意力、PyTorch SDPA、xFormers（可选）；
- 指标：峰值显存、p50/p99 延迟、吞吐（TFLOPs 近似）、数值误差；
- 工具：`torch.cuda.memory_allocated`、`nsys/ncu`、自写 microbench。

------

## 10. 变体与进阶主题

- FlashAttention-2/3 思路与差异（短序列/warp 专化/TMA 等）概览；
- 训练特性：Dropout、Bias、RoPE、ALiBi；
- 架构兼容：GQA/MQA、多查询解码、KV-Cache 与 **FlashDecoding**；
- 任务特化：滑动窗口注意力、分块稀疏（block-sparse）。

------

## 11. 常见坑位 & 调试清单

- 因果掩码边界越界；
- `exp` 下溢/Inf；
- register 溢出导致 occupancy 降低；
- SMEM bank conflict；
- Triton meta-params 搜索不充分；
- 反向数值不稳（重构 P 的公式/顺序错误）。

------

## 12. 验证与交付清单

- ✅ 前向：与基线逐元素 ≤1e-5（FP32）/ ≤1e-3（FP16/BF16）
- ✅ 反向：`gradcheck` 通过；随机形状梯度相对误差统计表；
- ✅ 性能：在 4k–8k 序列显存显著下降、延迟显著优于朴素与接近/优于 SDPA（视硬件而定）。

------

## 13. 下一步建议

1. 按第 3 节先写出 **PyTorch 分块前向原型**（含因果掩码）。
2. 写 `pytest` 单测对齐朴素注意力结果；加随机化与长序列样例。
3. 然后进入第 4 节的 **Triton 前向 kernel**。

> 需要我在下一步直接给出：**(a) 朴素注意力 baseline、(b) 分块在线 softmax 原型（PyTorch 版）** 的最小可运行代码与单测骨架吗？