好！先给你一份“从零到深入实现”的 PagedAttention 学习大纲。按这个路线我们可以一章一章慢慢讲、边讲边画小例子与伪代码，最后给出工程化 checklist（以 vLLM 为主线、兼顾通用实现）。

# PagedAttention 学习大纲

## 0. 预备知识与问题背景

- 0.1 回顾：自注意力与 KV Cache 的基本形态（prefill vs decode 两阶段）
- 0.2 现实瓶颈：长上下文、并发请求导致的显存碎片化与浪费
- 0.3 基础对比：传统“连续KV缓存” vs 分页化（paged/block）KV 缓存

## 1. 核心理念：把 KV Cache 做成“分页内存”

- 1.1 逻辑序列与物理页的解耦：logical tokens → page table → physical blocks
- 1.2 Page / Block 的粒度选择：block_size（例如每块含若干 token 的 KV 槽位）取舍
- 1.3 结构设计：per-request 的 block table、全局 block pool、空闲链表/位图
- 1.4 目标：减少碎片、支持复用/扩展、提升多并发吞吐

## 2. 数据布局与索引

- 2.1 KV 的张量排布：[layers, heads, blocks, block_size, head_dim] 的几种实现方案
- 2.2 跨 head 的打包与对齐（MHA / MQA / GQA 的差异）
- 2.3 跨层分配 vs 逐层分配（每层独立页表？共享池？）
- 2.4 索引路径：token idx → (seq_id, page_id, offset) → 物理地址（gather）

## 3. 分配器与内存管理

- 3.1 Block allocator 设计：首次适配/最佳适配、位图、环形缓冲、批量分配
- 3.2 扩容、回收与复用：append token 时如何摊还分配成本
- 3.3 碎片与整理：何时 compaction？如何避免停顿（最好不做或增量做）
- 3.4 跨请求共享：Prompt/前缀页共享与写时复制（COW）

## 4. 推理流程中的 paged 访问

- 4.1 Prefill 阶段（批量新建序列）：页的批量分配与顺序写入
- 4.2 Decode 阶段（step-by-step）：按需扩展、定位最后一页、写入新 KV
- 4.3 Beam Search 与分叉：共享前缀的页复用、分叉时的 COW 策略
- 4.4 Speculative Decoding 交互：撤销/确认时的页操作

## 5. 注意力核（kernel）如何“分页化”

- 5.1 标准注意力与 FlashAttention 的回顾（流式 softmax / tile 化）
- 5.2 PagedAttention 的访存策略：对 K/V 进行 block-wise gather
- 5.3 kernel 输入组织：block_table → 索引列表 → 批量 gather → kernel 计算
- 5.4 计算/访存权衡：块越小越灵活，但 gather 开销↑；块越大 locality↑ 但碎片↑
- 5.5 与 FlashAttention 的关系：FA 解决算子级 IO/数值稳定，Paged 解决全局内存布局；二者可叠加

## 6. 并发与调度：让分页优势变成吞吐

- 6.1 请求批处理（batching）与重复前缀合并（prefix merging）
- 6.2 “页”为单位的可抢占与调度（短请求优先、SLO 友好）
- 6.3 乱序序列的统一访问：页表让乱序也能高效 gather
- 6.4 多租户/多模型：池化、配额与隔离

## 7. 与 MQA/GQA、长上下文、滑动窗的结合

- 7.1 MHA vs MQA vs GQA：paged 下的存储量与带宽对比
- 7.2 Sliding-window / StreamingLLM：窗口滚动下的页淘汰/重用
- 7.3 超长上下文：分层页表、分段换入/换出策略

## 8. “出显存”策略：CPU/主存/显存的分层

- 8.1 冷页下放：Pinned memory / UVM / NVLink 机型的取舍
- 8.2 何时换入/换出：基于步数、冷热计数、启发式/代价模型
- 8.3 异步 DMA pipeline：overlap compute & copy

## 9. 精度与压缩

- 9.1 KV Cache 量化（FP16/FP8/INT8）：带来的容量×吞吐变化
- 9.2 per-head/per-layer 异构精度（热点高精，冷点低精）
- 9.3 数值稳定性与重排（layout 变化对 kernel 的影响）

## 10. 掩码与特殊场景

- 10.1 因果掩码在分块下的实现（块间上三角）
- 10.2 前缀缓存（prompt cache）与检索复用
- 10.3 MoE：不同专家分支的 KV 页组织
- 10.4 多段输入（工具调用/函数调用）与 segment 边界

## 11. 多卡与并行策略

- 11.1 张量并行下的 paged：每卡保存自身份额的 KV 页
- 11.2 流水并行/分层并行：层切分后页表的分摊与路由
- 11.3 跨节点：页的放置策略与远程访问（尽量避免）

## 12. 实现细节（工程视角）

- 12.1 关键数据结构：BlockPool、FreeList/Bitmap、PageTable、SeqState
- 12.2 关键 API：alloc_block(s)、append_token、fork_seq、free_blocks
- 12.3 Kernel 前后的 packing/unpacking：gather list 生成与缓存
- 12.4 监控与调优：碎片率、命中率、page-gather 带宽、分配抖动
- 12.5 失败注入与极端场景：超长序列/海量并发/频繁分叉

## 13. 与 vLLM 的对照与实战

- 13.1 vLLM 中的 BlockManager / Scheduler 轮廓
- 13.2 前缀缓存与 Paged KV 的结合路径
- 13.3 典型配置：block_size 选型、并发批量、pin-memory/量化开关
- 13.4 压测方法：QPS/TTFT/TPOT、上下文长度扫描、并发曲线

## 14. 常见坑与权衡

- 14.1 小块导致的 gather 过多（kernel 吞吐下降）
- 14.2 大块导致的内存浪费与灵活性下降
- 14.3 COW 时机/粒度选择（复制风暴 vs 共享冲突）
- 14.4 长尾序列“霸占”页：分级淘汰与限流

## 15. 最终 Checklist（落地指南）

- 15.1 需求画像：场景、并发、上下文分布
- 15.2 设计落点：block_size、量化、调度策略、是否跨层共享
- 15.3 实施步骤：先页化 KV → 接 Page-aware kernel → 做前缀复用 → 冷页下放
- 15.4 验证：对照“连续KV缓存”的等价性与收益（显存、吞吐、稳定性）

------

如果你认可这个目录，我们从**第 0 章**开始：先把“为什么需要 PagedAttention（痛点）”讲透，再画一个最小可用的数据结构草图，随后进入分配器与 kernel 的分页化访问。