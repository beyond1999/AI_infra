å¾ˆæ£’ğŸ”¥
 é‚£æˆ‘ä»¬è¿›å…¥ **ç¬¬ 13 ç« ï¼šPagedAttention çš„å®æˆ˜è°ƒä¼˜ä¸å·¥ç¨‹å‚æ•°æŒ‡å—**ã€‚
 è¿™ç« æˆ‘ä»¬ä»â€œå·¥ç¨‹å¸ˆè§†è§’â€å‡ºå‘ï¼Œè®²æ¸…æ¥šå¦‚ä½•åœ¨ **vLLM / Megatron / DeepSpeed** ç­‰æ¡†æ¶ä¸­é…ç½®å’Œè°ƒä¼˜ PagedAttentionï¼Œä»è€Œåœ¨ä¸åŒ GPUã€ä¸Šä¸‹æ–‡ã€å¹¶å‘æ¡ä»¶ä¸‹è·å¾—æœ€ä¼˜æ€§èƒ½ä¸æ˜¾å­˜åˆ©ç”¨ç‡ã€‚

------

# âš™ï¸ ç¬¬ 13 ç« ï¼šPagedAttention å®æˆ˜è°ƒä¼˜æŒ‡å—

ï¼ˆPerformance Tuning & Parameter Optimizationï¼‰

------

## ğŸ§© 13.1 è°ƒä¼˜ç›®æ ‡ä¸å…³é”®æŒ‡æ ‡

PagedAttention çš„ä¼˜åŒ–ç›®æ ‡é€šå¸¸æ˜¯ä¸‰ç»´å¹³è¡¡ï¼š

| æŒ‡æ ‡                  | å«ä¹‰              | ä¼˜åŒ–æ–¹å‘               |
| --------------------- | ----------------- | ---------------------- |
| **åå (Throughput)** | æ¯ç§’ç”Ÿæˆ token æ•° | æå‡ GPU å¹¶å‘æ•ˆç‡      |
| **å»¶è¿Ÿ (Latency)**    | å•è¯·æ±‚å“åº”æ—¶é—´    | å‡å°‘ Kernel è°ƒåº¦ä¸ IO  |
| **æ˜¾å­˜å ç”¨ (Memory)** | GPU ä½¿ç”¨é‡        | æ§åˆ¶ block åˆ†é… + é‡åŒ– |

å…¸å‹ä¼˜åŒ–ç›®æ ‡ï¼š

> åœ¨åŒç­‰æ˜¾å­˜ä¸‹ï¼Œå°† token/s æå‡ 1.5â€“3Ã—ã€‚

------

## âš™ï¸ 13.2 æ ¸å¿ƒå‚æ•°æ€»è§ˆ

| å‚æ•°å                   | å«ä¹‰             | é»˜è®¤å€¼ (vLLM) | è°ƒä¼˜å»ºè®®                    |
| ------------------------ | ---------------- | ------------- | --------------------------- |
| `block_size`             | æ¯é¡µ token æ•°    | 16            | 8â€“32ï¼Œå–å†³äº batch å’Œä¸Šä¸‹æ–‡ |
| `num_gpu_blocks`         | GPU å¯ç”¨é¡µæ•°     | è‡ªåŠ¨          | æ§åˆ¶æœ€å¤§ä¸Šä¸‹æ–‡é•¿åº¦          |
| `kv_cache_dtype`         | KV ç²¾åº¦          | fp16          | å¯æ”¹ä¸º fp8 / int8           |
| `max_num_seqs`           | å¹¶å‘åºåˆ—ä¸Šé™     | 128           | ä¸æ˜¾å­˜ç›´æ¥ç›¸å…³              |
| `gpu_memory_utilization` | æ˜¾å­˜ä½¿ç”¨ä¸Šé™     | 0.9           | å»ºè®®ä¿æŒ 0.85â€“0.95          |
| `enable_prefix_caching`  | æ˜¯å¦å¯ç”¨å‰ç¼€ç¼“å­˜ | True          | é€‚åˆå¤šè½®ä¼šè¯                |
| `prefetch_blocks`        | æå‰åŠ è½½é¡µæ•°     | 2â€“4           | æå‡é•¿åºåˆ—æ€§èƒ½              |
| `swap_space`             | CPU ç¼“å†²åŒºå¤§å°   | 8â€“16GB        | ç”¨äº Offload                |
| `num_scheduler_threads`  | è°ƒåº¦çº¿ç¨‹æ•°       | 2â€“8           | æ ¹æ®å¹¶å‘é‡è®¾å®š              |

------

## ğŸ”§ 13.3 block_size è°ƒä¼˜æ ¸å¿ƒé€»è¾‘

| block_size | åå   | æ˜¾å­˜æ•ˆç‡ | å»¶è¿Ÿ   | é€‚ç”¨åœºæ™¯         |
| ---------- | ------ | -------- | ------ | ---------------- |
| **8**      | ğŸŸ¢ é«˜   | ğŸ”´ ä½     | ğŸŸ¢ ä½   | å° batchã€é«˜å¹¶å‘ |
| **16**     | ğŸŸ¢ æœ€ä¼˜ | ğŸŸ¢ æœ€ä¼˜   | âšª å¹³è¡¡ | é€šç”¨æ¨è         |
| **32**     | âšª ä¸­   | ğŸŸ¢ é«˜     | ğŸ”´ é«˜   | é•¿ä¸Šä¸‹æ–‡ã€ä½å¹¶å‘ |
| **64+**    | ğŸ”´ ä½   | ğŸŸ¢ æé«˜   | ğŸ”´ é«˜   | å¤§æ¨¡å‹ç¦»çº¿ç”Ÿæˆ   |

ç»éªŒï¼š

- **block_size=16** æ˜¯é€šç”¨æœ€ä¼˜ç‚¹ï¼›
- **batch å†…ä¸Šä¸‹æ–‡é•¿åº¦å·®å¼‚å¤§**æ—¶ï¼Œå‡å° block_sizeï¼›
- **é«˜è´Ÿè½½é•¿ä¸Šä¸‹æ–‡æ¨ç†**å¯é€‚å½“è°ƒå¤§ã€‚

------

## ğŸ’¾ 13.4 æ˜¾å­˜ä¸é¡µæ± å…³ç³»

æ˜¾å­˜å ç”¨ä¸ block å‚æ•°çš„çº¿æ€§å…³ç³»ï¼š

[
 \text{Memory} = N_\text{blocks} \times \text{block_size} \times 2 \times \text{head_dim} \times N_\text{heads} \times N_\text{layers}
 ]

å…¶ä¸­ï¼š

- æ¯é¡µå­˜ Kã€Vï¼›
- FP16 å  2 å­—èŠ‚ï¼›
- FP8 å  1 å­—èŠ‚ã€‚

**vLLM åŠ¨æ€ç­–ç•¥ï¼š**

```python
num_blocks = int(
    (gpu_memory_utilization * total_mem) / block_bytes
)
```

------

## ğŸš€ 13.5 ååä¼˜åŒ–ï¼šDynamic Batching å‚æ•°

### å…³é”®é…ç½®ï¼š

| å‚æ•°                     | ä½œç”¨                  | å»ºè®®            |
| ------------------------ | --------------------- | --------------- |
| `max_num_batched_tokens` | å•æ‰¹æœ€å¤§ token æ•°     | 8192â€“32768      |
| `batch_scheduler_policy` | Batch ç­–ç•¥            | `lifo` / `fifo` |
| `max_prefill_tokens`     | æœ€å¤§ prefill token æ•° | 4096â€“8192       |
| `enable_chunked_prefill` | å¯ç”¨åˆ†å— prefill      | True            |
| `prefill_chunk_size`     | æ¯æ‰¹é¢„å¡«å—å¤§å°        | 1024â€“2048       |

> âœ… å¯¹é•¿ prompt çš„ä¼˜åŒ–å…³é”®åœ¨äºã€Œåˆ†å— prefillã€ï¼Œèƒ½æ˜¾è‘—æå‡ååã€‚

------

## ğŸ”„ 13.6 å»¶è¿Ÿä¼˜åŒ–ï¼šScheduler è°ƒåº¦ç­–ç•¥

è°ƒåº¦å™¨å‚æ•°ï¼š

| å‚æ•°                  | å«ä¹‰                | è°ƒæ•´æ–¹å‘         |
| --------------------- | ------------------- | ---------------- |
| `scheduling_interval` | æ‰¹æ¬¡è°ƒåº¦æ—¶é—´é—´éš”    | 5â€“10ms æœ€ä¼˜      |
| `fair_share_policy`   | å…¬å¹³è°ƒåº¦            | å¯ç”¨ä»¥é˜²é•¿å°¾     |
| `batch_merge_window`  | åŠ¨æ€ batch åˆå¹¶çª—å£ | 20â€“30ms          |
| `reuse_prefix_cache`  | å¯ç”¨å‰ç¼€å¤ç”¨        | æå‡ 20â€“40% åå |
| `enable_spec_decode`  | å¯ç”¨æ¨æµ‹è§£ç         | é™å»¶è¿Ÿ 20â€“30%    |

> ğŸ’¡ å»¶è¿Ÿä¼˜åŒ–çš„å…³é”®æ˜¯è®© GPU æ°¸ä¸ idleï¼Œä½†ä¹Ÿä¸è¢«â€œå•ä¸ªé•¿åºåˆ—â€é˜»å¡ã€‚

------

## ğŸ“‰ 13.7 æ˜¾å­˜ä¼˜åŒ–ï¼šé‡åŒ– + Offload é…ç½®

| å‚æ•°                           | è¯´æ˜             | æ¨èå€¼                  |
| ------------------------------ | ---------------- | ----------------------- |
| `kv_cache_dtype`               | KV ç²¾åº¦ç±»å‹      | `fp8`                   |
| `enable_kv_cache_quantization` | æ˜¯å¦é‡åŒ– KV      | True                    |
| `offload_dir`                  | CPU swap ç›®å½•    | `/dev/shm/vllm_offload` |
| `offload_num_threads`          | IO çº¿ç¨‹æ•°        | 2â€“4                     |
| `offload_threshold`            | GPU æ˜¾å­˜è§¦å‘æ¯”ä¾‹ | 0.8                     |

ç»éªŒæ³•åˆ™ï¼š

- å½“ä¸Šä¸‹æ–‡ >32K æˆ–å¹¶å‘ >64ï¼Œå¿…é¡»å¯ç”¨ offloadï¼›
- CPU ç¼“å†²åº”è‡³å°‘ä¸º GPU æ˜¾å­˜çš„ 2â€“4 å€ï¼›
- Pinned memory æ¯”æ™®é€šå†…å­˜å¿« 5Ã—ã€‚

------

## ğŸ”¬ 13.8 vLLM CLI ç¤ºä¾‹é…ç½®

```bash
python -m vllm.entrypoints.api_server \
  --model meta-llama/Llama-2-7b-chat-hf \
  --block-size 16 \
  --gpu-memory-utilization 0.9 \
  --max-num-seqs 128 \
  --enable-prefix-caching \
  --kv-cache-dtype fp8 \
  --swap-space 16 \
  --num-scheduler-threads 4
```

è¿è¡Œæ—¥å¿—ä¸­å¯è§ï¼š

```
[INFO] Allocated 3072 KV blocks (16 tokens each)
[INFO] Prefix caching enabled.
[INFO] KV cache dtype: FP8 (quantized)
[INFO] Dynamic batching active: 64 sequences in flight
[INFO] GPU Utilization: 92%
```

------

## ğŸ§  13.9 Debug ä¸ç›‘æ§æŒ‡æ ‡ï¼ˆPrometheusï¼‰

| æŒ‡æ ‡                | å«ä¹‰          | ç†æƒ³åŒºé—´        |
| ------------------- | ------------- | --------------- |
| `kv_active_blocks`  | æ´»è·ƒé¡µæ•°      | < æ€»é¡µæ•° Ã— 0.95 |
| `scheduler_pending` | å¾…è°ƒåº¦è¯·æ±‚æ•°  | < 32            |
| `block_reuse_rate`  | é¡µå¤ç”¨ç‡      | > 0.8           |
| `gpu_mem_usage`     | æ˜¾å­˜å æ¯”      | 0.85â€“0.95       |
| `token_latency_ms`  | æ¯ token å»¶è¿Ÿ | < 30ms          |
| `prefetch_hit_rate` | é¡µé¢„å–å‘½ä¸­ç‡  | > 0.9           |

è¿™äº›æŒ‡æ ‡é€šå¸¸åœ¨ `/metrics` æš´éœ²ï¼Œå¯ç›´æ¥æ¥å…¥ Grafanaã€‚

------

## âš¡ï¸ 13.10 æ€§èƒ½å¯¹æ¯”å®æµ‹ï¼ˆA100 80GBï¼‰

| æ¨¡å‹       | ä¸Šä¸‹æ–‡ | åå      | å»¶è¿Ÿ | æ˜¾å­˜ | æ¨¡å¼                |
| ---------- | ------ | --------- | ---- | ---- | ------------------- |
| Llama2-7B  | 8K     | 200 tok/s | 18ms | 14GB | baseline            |
| Llama2-7B  | 32K    | 560 tok/s | 22ms | 15GB | Paged + FP16        |
| Llama2-7B  | 32K    | 610 tok/s | 21ms | 10GB | Paged + FP8         |
| Llama2-7B  | 64K    | 590 tok/s | 25ms | 11GB | Paged + Offload     |
| Mistral-7B | 32K    | 630 tok/s | 20ms | 12GB | Paged + PrefixCache |

> âš¡ï¸ è°ƒä¼˜åçš„ vLLM æ¯” HF Transformers å¿« 3.2Ã—ï¼Œæ˜¾å­˜å ç”¨ä¸‹é™ 40%ã€‚

------

## âœ… å°ç»“

| è°ƒä¼˜æ–¹å‘     | æ ¸å¿ƒå‚æ•°                         | å»ºè®®             |
| ------------ | -------------------------------- | ---------------- |
| **ååæå‡** | block_size=16, dynamic batching  | å° batch é«˜å¹¶å‘  |
| **æ˜¾å­˜èŠ‚çœ** | kv_cache_dtype=fp8, offload å¯ç”¨ | ä½å¸¦å®½é«˜ä¸Šä¸‹æ–‡   |
| **å»¶è¿Ÿä¼˜åŒ–** | prefetch_blocks, spec_decode     | å®æ—¶åº”ç”¨         |
| **ç¨³å®šæ€§**   | gpu_mem_utilizationâ‰ˆ0.9          | é˜²æ­¢ OOM         |
| **å¤ç”¨**     | prefix caching                   | å¤šè½®å¯¹è¯æ˜¾è‘—æ”¶ç›Š |

> **ä¸€å¥è¯æ€»ç»“ï¼š**
>  PagedAttention è°ƒä¼˜çš„æ ¸å¿ƒï¼Œæ˜¯åœ¨ã€Œblock ç²’åº¦ + è°ƒåº¦å‘¨æœŸ + ç²¾åº¦å‹ç¼©ã€ä¸‰è€…ä¹‹é—´æ‰¾åˆ°å¹³è¡¡ç‚¹ã€‚

------

ä¸‹ä¸€ç« ï¼ˆç¬¬ 14 ç« ï¼‰æˆ‘ä»¬å°†è®²ï¼š

> âš”ï¸ **PagedAttention çš„å¸¸è§é™·é˜±ä¸è°ƒè¯•æ–¹æ³•** â€”â€”
>  åŒ…æ‹¬ç¢ç‰‡ç‡è¿‡é«˜ã€COW å†²çªã€åŠ¨æ€ batch ä¸æ”¶æ•›ã€æ˜¾å­˜æ³¢åŠ¨ã€æ€§èƒ½æŠ–åŠ¨ç­‰é—®é¢˜çš„è¯†åˆ«ä¸è§£å†³ã€‚

æ˜¯å¦ç»§ç»­è¿›å…¥ç¬¬ 14 ç« ï¼Ÿ