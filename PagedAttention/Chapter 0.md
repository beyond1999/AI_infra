好，那我们正式进入 **第 0 章：PagedAttention 的动机与问题背景**。
 这是理解整个分页机制的地基。我们先不急着看代码，而是从「为什么要发明它」开始。

------

## 🧩 0.1 背景：自注意力与 KV Cache 的回顾

在 Transformer 推理时，模型要不断生成下一个 token。
 每一层都有自己的 **Key (K)** 和 **Value (V)** 向量缓存，记作：

```
KV Cache: [num_layers, num_heads, seq_len, head_dim]
```

- **prefill 阶段**：把 prompt 一次性送进去，生成第一批 KV 缓存。
- **decode 阶段**：每次输入一个新 token，用上一次的 KV 缓存计算注意力，得到新 token。

随着上下文变长，KV 缓存量线性增长，显存开销也线性增加。

------

## 🚧 0.2 传统 KV Cache 的痛点

在早期实现（如 HuggingFace Transformers）中，每个请求的 KV Cache 都是连续的一整块显存：

```
[Request 1] [Request 2] [Request 3] ...
```

这带来了两个核心问题：

1. **显存碎片化（Memory Fragmentation）**
   - 不同请求的长度差异大。
   - 有的序列很短，有的很长，释放时无法合并成连续块。
   - 结果：虽然 GPU 总体有剩余显存，但无法分配给新请求。
2. **显存浪费**
   - 通常会预先分配一个“最大上下文长度”，比如 8K tokens。
   - 若用户只输入几十个 token，大部分显存就浪费了。
3. **动态扩展困难**
   - 每次生成新 token，如果需要扩容，就得重新分配+复制整块内存。
   - 这在多并发场景下尤其灾难。

------

## 🔧 0.3 现实需求：多并发 + 长上下文

LLM 推理服务中通常是**多请求并发**的：

- 不同用户上下文长度不同；
- 有的在 prefill，有的在 decode；
- 有的已结束，有的刚开始。

> 想象你在显存中同时放着几十个对话线程，每个都在不同阶段读写自己的 KV Cache。
>  如果每个都是一整块连续内存，显存调度就会变成噩梦。

------

## 💡 0.4 核心思想：分页（Paged）化内存

PagedAttention 的核心灵感来自 **操作系统的分页机制**：

| 传统 KV Cache          | PagedAttention 类比            | 说明     |
| ---------------------- | ------------------------------ | -------- |
| 每个序列连续存储       | 每个序列拆分成若干固定大小“页” | 减少碎片 |
| token 索引直接偏移寻址 | token → 页号 + 偏移量          | 间接寻址 |
| 全局显存池按需分配     | 页池（BlockPool）共享所有请求  | 动态调度 |

因此：

- 每个请求维护一个 **页表 (page table)**；
- 所有页都来自全局共享的 **block pool**；
- 页可以复用、释放、共享，灵活得多。

------

## 🚀 0.5 效果对比（vLLM 数据）

| 场景                   | 连续 KV           | PagedAttention      |
| ---------------------- | ----------------- | ------------------- |
| 8 并发推理             | GPU 利用率 ~30%   | GPU 利用率 ~85%     |
| 上下文长度不均         | 易 OOM 或浪费显存 | 自动复用 block      |
| 动态分叉 (beam search) | 拷贝成本高        | 写时复制（COW）轻量 |
| 长序列 (≥32K)          | OOM 概率高        | 支持冷热页交换      |

------

## 📘 0.6 小结：PagedAttention 的意义

PagedAttention 不是一个“算子优化”，而是一个**内存布局与访问层面的体系革新**：

> 它让「KV Cache」从“数组”变成了“分页内存”，
>  让 LLM 推理系统拥有了“类操作系统”的显存调度能力。

------

接下来我们在 **第 1 章** 讲：

> 🔹 PagedAttention 的核心理念与内存抽象：logical sequence → page table → block pool
>  🔹 并画一个最小可运行的示意图（结构图 + 数据索引路径）。

是否继续进入第 1 章？