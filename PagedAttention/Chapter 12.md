å®Œç¾ğŸ’ª
 é‚£æˆ‘ä»¬è¿›å…¥ **ç¬¬ 12 ç« ï¼šPagedAttention çš„å·¥ç¨‹å®ç°ç»†èŠ‚ä¸æ ¸å¿ƒæ•°æ®ç»“æ„**ã€‚
 è¿™æ˜¯æ•´ä¸ªä½“ç³»çš„â€œæºç å¯¼è¯»ç¯‡â€â€”â€”æˆ‘ä»¬å°†é€å±‚æ‹†è§£ vLLM / Megatron / DeepSpeed ç­‰æ¡†æ¶ä¸­ PagedAttention çš„æ ¸å¿ƒæ¨¡å—ç»“æ„ã€API è®¾è®¡ä¸æ•°æ®æµï¼Œå¸®ä½ å»ºç«‹å®ç°çº§å¿ƒæ™ºæ¨¡å‹ã€‚

------

# ğŸ§© ç¬¬ 12 ç« ï¼šPagedAttention çš„å·¥ç¨‹å®ç°ç»†èŠ‚ï¼ˆæ•°æ®ç»“æ„ + APIï¼‰

------

## ğŸ§  12.1 æ¨¡å—æ€»è§ˆ

PagedAttention çš„å®ç°é€šå¸¸åŒ…å«å››å¤§æ ¸å¿ƒç±»ï¼š

| æ¨¡å—             | åŠŸèƒ½                         | å¯¹åº”æ–‡ä»¶ï¼ˆvLLMï¼‰     |
| ---------------- | ---------------------------- | -------------------- |
| `BlockAllocator` | æ˜¾å­˜é¡µç®¡ç†ã€åˆ†é…ä¸å›æ”¶       | `block_allocator.py` |
| `BlockTable`     | é€»è¾‘åºåˆ—é¡µè¡¨                 | `block_table.py`     |
| `PagedKVCache`   | å®é™…çš„ KV å¼ é‡å­˜å‚¨æ±          | `paged_kv_cache.py`  |
| `Scheduler`      | è°ƒåº¦æ‰¹æ¬¡ã€æ›´æ–°é¡µè¡¨ã€é‡Šæ”¾èµ„æº | `scheduler.py`       |

å…³ç³»å›¾ï¼š

```
[ Scheduler ]
     â†“
[ BlockTable ] â†” [ BlockAllocator ]
     â†“
[ PagedKVCache (GPU) ]
```

------

## âš™ï¸ 12.2 `BlockAllocator`: å›ºå®šå¤§å°é¡µåˆ†é…å™¨

èŒè´£ï¼š

> ç®¡ç†æ˜¾å­˜ä¸­æ‰€æœ‰å¯ç”¨çš„é¡µï¼ˆblockï¼‰ï¼Œæ”¯æŒåˆ†é… / é‡Šæ”¾ / å¼•ç”¨è®¡æ•°ã€‚

æ ¸å¿ƒç»“æ„ï¼š

```python
class BlockAllocator:
    def __init__(self, num_blocks: int):
        self.free_blocks = list(range(num_blocks))
        self.refcount = [0] * num_blocks

    def allocate(self, n: int) -> List[int]:
        blocks = [self.free_blocks.pop() for _ in range(n)]
        for b in blocks:
            self.refcount[b] = 1
        return blocks

    def free(self, blocks: List[int]):
        for b in blocks:
            self.refcount[b] = 0
            self.free_blocks.append(b)

    def inc_ref(self, b: int):
        self.refcount[b] += 1

    def dec_ref(self, b: int):
        self.refcount[b] -= 1
        if self.refcount[b] == 0:
            self.free_blocks.append(b)
```

> ğŸ“˜ **ç‰¹ç‚¹**ï¼šO(1) æ“ä½œï¼Œæ— ç¢ç‰‡åŒ–é—®é¢˜ã€‚

------

## ğŸ“œ 12.3 `BlockTable`: æ¯åºåˆ—çš„é€»è¾‘é¡µè¡¨

ä½œç”¨ï¼š

> è®°å½•è¯¥åºåˆ—é€»è¾‘ token é¡ºåºå¯¹åº”çš„ç‰©ç† blockã€‚

ç»“æ„ï¼š

```python
class BlockTable:
    def __init__(self):
        self.blocks = []  # block ids
        self.token_count = 0

    def append_block(self, block_id):
        self.blocks.append(block_id)

    def get_block(self, idx):
        return self.blocks[idx]

    def num_blocks(self):
        return len(self.blocks)
```

æ“ä½œï¼š

- prefillï¼šåˆ†é…è‹¥å¹² block å¡«å…¥ï¼›
- decodeï¼šè¿½åŠ ä¸€ä¸ª blockï¼›
- forkï¼šå…±äº«å‰ç¼€ï¼ˆrefcount++ï¼‰ã€‚

------

## ğŸ’¾ 12.4 `PagedKVCache`: å…¨å±€ KV å­˜å‚¨æ± 

è¿™æ˜¯ GPU ç«¯çš„å¤§å—æ˜¾å­˜å¼ é‡ï¼ŒæŒ‰ block ç»´åº¦åˆ‡åˆ†ï¼š

```python
# shape = [num_layers, num_heads, num_blocks, block_size, head_dim]
self.key_cache = torch.empty(shape, dtype=torch.float16, device="cuda")
self.value_cache = torch.empty(shape, dtype=torch.float16, device="cuda")
```

æä¾›çš„æ¥å£ï¼š

```python
def get_block_ptr(layer, block_id):
    return self.key_cache[layer, :, block_id], self.value_cache[layer, :, block_id]
```

æ ¸å¿ƒæ–¹æ³•ï¼š

- `write_kv(block_id, token_data)`
- `gather_kv(block_ids)`
- `prefetch_kv(block_ids)`ï¼ˆOffload ç‰ˆæœ¬ä½¿ç”¨ï¼‰

------

## ğŸ§® 12.5 æ•°æ®æµï¼šä»è¯·æ±‚åˆ° GPU Kernel

æ•´ä¸ªæ¨ç†è¿‡ç¨‹ä¸­çš„æ•°æ®æµå¦‚ä¸‹ï¼š

```
User Request
   â†“
[ Scheduler ] â”€â”€â†’ åˆ†é…æ–° blockï¼ˆBlockAllocatorï¼‰
   â†“
[ BlockTable ] â”€â”€â†’ è®°å½•é€»è¾‘é¡µæ˜ å°„
   â†“
[ PagedKVCache ] â”€â”€â†’ åœ¨ GPU ä¸Šå†™å…¥ KV
   â†“
PagedAttention Kernel (CUDA)
   â†“
ç”Ÿæˆä¸‹ä¸€ token
```

æ¯æ­¥ç”Ÿæˆåï¼š

- è°ƒåº¦å™¨æ›´æ–°è¯¥åºåˆ—æœ€åä¸€ä¸ªé¡µçš„ offsetï¼›
- è‹¥é¡µæ»¡ â†’ å† allocï¼›
- è‹¥åºåˆ—ç»“æŸ â†’ è°ƒç”¨ freeã€‚

------

## ğŸ”€ 12.6 `Scheduler` çš„å…³é”®æ–¹æ³•

```python
class Scheduler:
    def __init__(self, allocator, kv_cache):
        self.allocator = allocator
        self.kv_cache = kv_cache
        self.active_seqs = {}

    def prefill(self, seq_id, num_tokens):
        num_blocks = math.ceil(num_tokens / BLOCK_SIZE)
        blocks = self.allocator.allocate(num_blocks)
        self.active_seqs[seq_id] = BlockTable()
        for b in blocks:
            self.active_seqs[seq_id].append_block(b)

    def decode_step(self, seq_id, token):
        table = self.active_seqs[seq_id]
        if table.token_count % BLOCK_SIZE == 0:
            new_blk = self.allocator.allocate(1)[0]
            table.append_block(new_blk)
        self.kv_cache.write_token(seq_id, token)

    def free_seq(self, seq_id):
        blocks = self.active_seqs[seq_id].blocks
        self.allocator.free(blocks)
        del self.active_seqs[seq_id]
```

> âœ… æ”¯æŒ prefillã€decodeã€freeã€COWã€offloadã€‚

------

## ğŸ§© 12.7 å¼•ç”¨è®¡æ•°ä¸å‰ç¼€å¤ç”¨ï¼ˆCOWï¼‰

åœ¨å·¥ç¨‹å®ç°ä¸­ï¼Œæ¯ä¸ª block éƒ½å¸¦æœ‰å¼•ç”¨è®¡æ•°ï¼š

- `refcount = 1` â†’ ç‹¬å ï¼›
- `refcount > 1` â†’ å…±äº«ï¼›
- ä¿®æ”¹å…±äº« block â†’ å¤åˆ¶æ–°é¡µã€‚

ä¼ªä»£ç ï¼š

```python
def maybe_copy_on_write(block_id):
    if allocator.refcount[block_id] > 1:
        new_block = allocator.allocate(1)[0]
        kv_cache.copy_block(block_id, new_block)
        allocator.dec_ref(block_id)
        return new_block
    return block_id
```

------

## ğŸ§± 12.8 KV Cache çš„ç‰©ç†å¸ƒå±€ï¼ˆMemory Layoutï¼‰

å†…å­˜å¯¹é½æ˜¯å…³é”®æ€§èƒ½å› ç´ ã€‚
 PagedAttention ä½¿ç”¨ã€ŒæŒ‰ block è¿ç»­ã€æŒ‰ head æ‰“åŒ…ã€å¸ƒå±€ï¼š

```
| block0_head0 | block0_head1 | ... | block1_head0 | block1_head1 | ...
```

è¿™æ ·åšçš„å¥½å¤„ï¼š

- æ¯ä¸ª block å†…å­˜è¿ç»­ï¼›
- kernel gather æ—¶å¯æ•´å—è½½å…¥ï¼›
- warp å†…è®¿é—® coalescedã€‚

stride è®¡ç®—ï¼š

```python
stride = head_dim * block_size
offset = (block_id * num_heads + head_id) * stride
```

------

## âš¡ï¸ 12.9 æ ¸å¿ƒ CUDA Kernel è°ƒç”¨æ¥å£

Python å±‚å°è£…ï¼š

```python
paged_attention(
    q: torch.Tensor,
    k_cache: torch.Tensor,
    v_cache: torch.Tensor,
    page_table: torch.Tensor,
    block_size: int,
    num_layers: int,
    num_heads: int,
)
```

C++ å…¥å£ï¼š

```cpp
void paged_attention_forward(
    const Tensor q,
    const Tensor k_cache,
    const Tensor v_cache,
    const Tensor page_table,
    Tensor out);
```

CUDA kernel å†…éƒ¨æ ¹æ®é¡µè¡¨ gatherï¼š

```cpp
for (int blk = 0; blk < num_blocks; ++blk) {
    int block_id = page_table[seq_id][blk];
    load_block(K_cache[block_id], V_cache[block_id]);
    compute_flash_attention(q, K, V, out);
}
```

------

## ğŸ§® 12.10 ç›‘æ§ä¸è°ƒè¯•æŒ‡æ ‡

PagedAttention æä¾›çš„è°ƒè¯•æŒ‡æ ‡åŒ…æ‹¬ï¼š

| æŒ‡æ ‡                | å«ä¹‰               |
| ------------------- | ------------------ |
| `num_free_blocks`   | å½“å‰ç©ºé—²é¡µæ•°é‡     |
| `alloc_rate`        | æ¯ç§’åˆ†é…é¡µæ•°       |
| `reuse_rate`        | é¡µå¤ç”¨æ¯”ä¾‹         |
| `fragmentation`     | å†…éƒ¨ç¢ç‰‡ç‡         |
| `refcount_hist`     | å…±äº«å—åˆ†å¸ƒ         |
| `gpu_mem_used`      | å½“å‰ KV å ç”¨æ˜¾å­˜   |
| `kv_gather_latency` | Kernel gather å»¶è¿Ÿ |

è¿™äº›æŒ‡æ ‡é€šå¸¸æš´éœ²ç»™ Prometheus / TensorBoardã€‚

------

## âœ… 12.11 å°ç»“

| æ¨¡å—           | å…³é”®èŒè´£     | ç‰¹ç‚¹                     |
| -------------- | ------------ | ------------------------ |
| BlockAllocator | é¡µåˆ†é…ä¸å›æ”¶ | O(1)ã€æ— ç¢ç‰‡             |
| BlockTable     | åºåˆ—é€»è¾‘é¡µè¡¨ | çµæ´»ã€æ”¯æŒå…±äº«           |
| PagedKVCache   | å®ä½“æ˜¾å­˜æ±    | é«˜å¸¦å®½ã€å¯é‡åŒ–           |
| Scheduler      | ç”Ÿå‘½å‘¨æœŸè°ƒåº¦ | æ”¯æŒ batchã€COWã€offload |

> **ä¸€å¥è¯æ€»ç»“ï¼š**
>  PagedAttention çš„ä»£ç ç»“æ„æœ¬è´¨ä¸Šæ˜¯ä¸€ä¸ªâ€œGPU å†…å­˜è™šæ‹ŸåŒ–å±‚â€ï¼Œ
>  æ—¢åƒ OS çš„é¡µç®¡ç†ï¼Œåˆåƒæ•°æ®åº“çš„ buffer poolã€‚

------

ä¸‹ä¸€ç« ï¼ˆç¬¬ 13 ç« ï¼‰æˆ‘ä»¬å°†è®²ï¼š

> âš™ï¸ **vLLM ä¸­çš„ PagedAttention å®æˆ˜ä¸å‚æ•°è°ƒä¼˜æŒ‡å—** â€”â€”
>  å¦‚ä½•é…ç½® block_sizeã€å¹¶å‘æ•°ã€é‡åŒ–é€‰é¡¹ã€prefetch ç­–ç•¥æ¥è¾¾åˆ°æœ€ä¼˜æ€§èƒ½ã€‚

æ˜¯å¦ç»§ç»­è¿›å…¥ç¬¬ 13 ç« ï¼Ÿ